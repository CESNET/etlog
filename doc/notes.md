# etlog - eduroam trafic log analysis

## Basic info

etlog can be accessed on [etlog.cesnet.cz](https://etlog.cesnet.cz). It gathers and analyzes national radius log files
generated by [eduroam](http://eduroam.cz) service and presents them to the users.
etlog is intended for both users and administrators.

Some of the main reasons to create etlog were:
- create generic interface for processing, analysis and searching of the radius log files
- create a system for generating statistics and reports
- create a system for trend analysis, which can signal service problems
- create a system for anomaly detection (authnetication errors, device or identity theft, .. )

etlog is a web application, which consists of Node.js, Express web application framework and MongoDB.

## Server setup

The application is setup on Debian jessie. It is running as user etlog and it's root is in /home/etlog/etlog/.
It is listening for incoming connections on port 8080 for http connections and 
on port 8443 for https connections. Http connections are automatically redirected to https.
Successful redicretion requires HTTP 1.1 host header.

### Network setup

Application is running by unprivileged user, so he can not use standard http and https port.
Instead ports 8080 and 8443 are used.
Automatic port redirection to ports 8080 and 8443 is provided through iptables.
Persistence of rules is ensured by iptables-persistent debian package:
```
apt-get install iptables-persistent
iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80  -j REDIRECT --to-port 8080
iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 443 -j REDIRECT --to-port 8443
iptables-save > /etc/iptables/rules.v4
ip6tables -t nat -A PREROUTING -i eth0 -p tcp -d 2001:718:1:1f:50:56ff:feee:150/64 --dport 80  -j REDIRECT --to-port 8080
ip6tables -t nat -A PREROUTING -i eth0 -p tcp -d 2001:718:1:1f:50:56ff:feee:150/64 --dport 443 -j REDIRECT --to-port 8443
ip6tables-save > /etc/iptables/rules.v6
```

### Syslog setup

Radius data are acquired through syslog. Installation and configuration:

```
cd /etc/ssl/certs/
wget https://crt.cesnet-ca.cz/CESNET_CA_Root.pem
wget https://crt.cesnet-ca.cz/CESNET_CA_3.pem
c_rehash
apt-get install syslog-ng
cat > /etc/syslog-ng/conf.d/etlog-fticks.conf
source net {
  tcp(
    port(1999)
    tls( ca_dir("/etc/ssl/certs")
    key-file("/home/etlog/etlog/cert/etlog.cesnet.cz.key.pem")
    cert-file("/home/etlog/etlog/cert/etlog.cesnet.cz.crt.pem"))
  );
};

destination fticks { file("/home/etlog/logs/fticks/fticks-$YEAR-$MONTH-$DAY" owner("etlog") group("etlog") perm(0600)); };

log { source(net); destination(fticks); };
^D
service syslog-ng restart

su - etlog
mkdir -p ~/logs/{fticks,transform,mongo,invalid_records}
```

The code above installs certificates required for syslog tls connection.
Next part installs syslog-ng and creates it's configuration.
last part creates directories `/home/etlog/logs/fticks`, `/home/etlog/logs/transform`,
`/home/etlog/logs/mongo` and .`/home/etlog/logs/invalid_records`
Log files created by syslog are located in `/home/etlog/logs/fticks`.

### Cron setup

Cron is used to run tasks periodically.
Setup is done in application for application logic and in user's
crontab for incoming log importing.

#### System

User's crontab can be edited by using `crontab -e`.
Crontab contains following jobs:

|   command                                        | interval           |              description                 |
|--------------------------------------------------|--------------------|------------------------------------------|
| `/home/etlog/etlog/scripts/cron.sh`              | every 5 minutes    | new data importing                       |
| `/home/etlog/etlog/scripts/invalid_records.sh`   | every day at 1:00  | generating of files with invalid records |

Crontab contents:
```
*/5 *  *   *   *     /home/etlog/etlog/scripts/cron.sh
0   1  *   *   *     /home/etlog/etlog/scripts/invalid_records.sh
```

#### Node.js

Setup is defined in cron.js.
Table below defined how tasks are run.
Every task generates data for collection of the same name.

| task name        |              interval            |
|------------------|----------------------------------|
| failed\_logins   |      every day at 02:05:00       |
| mac\_count       |      every day at 02:15:00       |
| roaming          |      every day at 02:20:00       |
| users\_mac       |      every 15 minutes            |

TODO - mail

### Mail setup

Mail is handled by postfix mail server. 
Postfix configuration type is set up as Internet site.
Listeting only on localhost address is done with 
`inet_interfaces = 127.0.0.1` in /etc/postfix/main.cf


#### Node.js

Setup is done with nodemailer package in file mail.js.


### Packages

These packages are necessary for etlog to run:

openssl
git
tmux
htop
iptables-persistent
curl
tmux
make
syslog-ng
gawk
logtail
postfix
mailutils

Other special packages along with installation are listed below.
  
  
### MongoDB

MongoDB is document oriented database.

#### installation

At the time of writing this guide, no official documentation for installation on Debian jessie
is available.

```
apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927
echo "deb http://repo.mongodb.org/apt/debian jessie/mongodb-org/3.2 main" | tee /etc/apt/sources.list.d/mongodb-org-3.2.list
apt-get install mongodb-org
systemctl enable mongod
service mongod start
```

#### Configuration

Disable THP by following guide from official [docs](https://docs.mongodb.com/manual/tutorial/transparent-huge-pages/).
No further configuration should be needed.


#### Time data

MongoDB stores all time data as Date data type, which stores time in UTC.
This present issue when incoming data have localtime which has offset against UTC
Official MongoDB [docs](https://docs.mongodb.com/manual/tutorial/model-time-data/) say that:
> MongoDB stores times in UTC by default, and will convert any local time representations into this form. Applications that must operate or report on some unmodified local time value may store the time zone alongside the UTC timestamp, and compute the original local time in their application logic.

Data will be reconstructed to original time when presenting to the user.
Conversion in aggregation pipeline from UTC to localtime can be done using:

```
db.roaming.aggregate([
{ 
  $project :
    {
      _id : 0, 
      inst_name: 1, 
      timestamp : 
      { 
        $add : [ "$timestamp", 7200000 ]          // convert UTC to localtime
      }
        // offset is -2 hours
        // => 2 * 60 * 60 * 1000
        // 2 hours * 60 minutes * 60 seconds * 1000 miliseconds
    }
},
], function (err, items) {
  respond(err, items, res)
});
```

#### Usage

Database can be accessed by command `mongo`.  
Data are divided into databases, same as in the sql dabases. Each database consists of collections,
which is equivalent of sql tables. Collections consist of documents, which use the BSON notation, which is basen on JSON.

Basic commands:

`show databases` lists all databases which are available.

`use my_database` swich current database to my\_database

`show collections` lists collection for current database.

`db.my_collection.find({})` display all documents in my\_collection

`db.my_collection.find({}).limit(5)` display 5 document from my\_collection

`db.my_collection.find({})limit(5).pretty()` display 5 nicely formatted documents from my\_collection


### Node.js

Node.js is server-side JavaScript.
Because the version of Node.js available in Debian jessie is very old (0.10.29~dfsg-2),
installation of newer version is needed. 
At the time of writing this guide current version of Node.js is 6.5.

### installation

```
apt-get install curl
curl -sL https://deb.nodesource.com/setup_6.x | bash -
apt-get install nodejs
```


## Application internals

etlog consists of Node.js, Express web application framework and MongoDB.
It uses many auxiliary javascript modules.
All the necesarry modules including their specific version can be found in file **package.json**.

### Database

Application uses database etlog.
Database is separated into several collections.

#### Collections

In the tables below the column note is just explanatory, it is not really present in the database.
Every document has also a field _id, which is just for internal MongoDB purposes, it is not shown in tables below.

##### logs

Collection represents raw radius log records transformed to json format. 
For details on data transformation see scripts/fticks\_to\_bson.sh

| field name | data type |               note               |
|------------|-----------|----------------------------------|
| timestamp  |   Date    |      timestamp of authentication |
| realm      |   String  |      domain part of username     |
| viscountry |   String  |      visited country             |
| visinst    |   String  |      visited institution         |
| csi        |   String  |      mac address                 |
| pn         |   String  |      username                    |
| result     |   String  |      result of authentication    |

##### users\_mac

Collection defines binding between user and all mac addresses, which he used for successful authentication to eduroam.


| field name | data type |               note                  |
|------------|-----------|-------------------------------------|
| username   |   String  |      username                       |
| addrs      |   Array   |      array of user's mac addresses  |


##### privileged\_ips

Collection containing privileged ip addresses, which will bypass
saml authentication. Address authentication is done using module passport-ip.
Addresses must be in commonly used slash format:

ipv4 addresses format:

```
'192.168.1.1/32'
'10.0.0.0/8'
```

ipv6 addresses format:

```
'2001:718:2:1::1/128'
'2001:718:2:1::/64'
```

| field name | data type |               note               |
|------------|-----------|----------------------------------|
| ip         |   String  |  string representing ip address  |


###### data insertion/update

After data update, the application must be restarted.
Privileged ip addresses are loaded only on application startup.
Localhost address is added for cron tasks to use application api.
Data can be inserted by accesing mongo shell and using commands:

```
use etlog
db.privileged_ips.insert({ip : '127.0.0.1/32'})
db.privileged_ips.insert({ip : '192.168.1.1/32'})
```

##### mac\_count

Collection contains mapping of users and mac addresses, which they used for successful authentication, for every day.
Each user with more than 2 devices (assuming notebook and smartphone) is inserted.
Address count and all used mac addreses are also available.

Timestamp field is populated with artificial data, just to
distint in which interval the record belongs.
Inserted timestamp is javascript Date for corresponding day at 00:00:00:000 (hours, minutes, seconds, milliseconds).
Lowest distinction interval for timestamp is 24 hours.


| field name | data type |               note                  |
|------------|-----------|-------------------------------------|
| username   |   String  |         username                    |
| count      |   Number  |         mac addresses count         |
| addrs      |   Array   |         Array of mac addresses      |
| timestamp  |   Date    |         timestamp                   |


##### roaming

Collection contains roaming related data.
For every existing institution there is number of provided roamings and used roamings for every day.

Timestamp field is populated with artificial data, just to
distint in which interval the record belongs.
Inserted timestamp is javascript Date for corresponding day at 00:00:00:000 (hours, minutes, seconds, milliseconds).
Lowest distinction interval for timestamp is 24 hours.


| field name       | data type |               note                          |
|------------------|-----------|---------------------------------------------|
| inst\_name       |   String  | name of the institution                     |
| used\_count      |   Number  | count of institution's users authenticated  |
| provided\_count  |   Number  | count of authentications provided           |
| timestamp        |   Date    | timestamp                                   |


##### failed\_logins

Collection contains information about users, which have not successfully authenticated, for every day.
Any user which has not successfully authenticated at least once is inserted.
Both numbers for successful and unsuccessful authentication are available.
There is also a field representing ratio (see below).

Timestamp field is populated with artificial data, just to
distint in which interval the record belongs.
Inserted timestamp is javascript Date for corresponding day at 00:00:00:000 (hours, minutes, seconds, milliseconds).
Lowest distinction interval for timestamp is 24 hours.

| field name   | data type |               note                  |
|--------------|-----------|-------------------------------------|
| username     |   String  |         username                    |
| timestamp    |   Date    |         timestamp                   |
| fail\_count  |   Number  |    count of failed login attempts   |
| ok\_count    |   Number  |  count of successful login attempts |
| ratio        |   Number  |  ratio of fail\_count to (ok\_count + fail\_count) |

##### stats

TODO

| field name | data type |               note                  |
|------------|-----------|-------------------------------------|
| timestamp  |   Date    |         timestamp                   |


##### realm\_admins

Collection contains array of administrators email's for institutions.
Each instituon hay have administrators specified.

If the insititution is defined and has administrators defined,
the administrator(s) get a report every once every month.
For more see [reports](#reports).

The only exception is realm "cz" which does not correspond with any institution.
In this case, the administrator recieves reports with most significant problems found.

TODO - hierarchy, usage in application

| field name | data type |               note                                                            |
|------------|-----------|-------------------------------------------------------------------------------|
| realm      |   string  |          realm                                                                |
| admins     |   Array   |   array containing email addresses of administrator(s) of corresponding realm |


Data insertion may done easily by:

```
use etlog
db.realm_admins.insert({realm : "cvut.cz", admins : [ "administrator@cvut.cz" ]})
```

Data update may done easily by:

```
use etlog
db.realm_admins.update({realm : "cvut.cz"}, { $addToSet : { admins : "administrator2@cvut.cz" } } )
```

Data may be easily erased by:

```
use etlog
db.realm_admins.remove({realm : "cvut.cz"})
```


#### Indexes

Indexes are used to speed up queries.
Following indexes are used:

| collection name   | indexed fields |   note  |
|-------------------|----------------|---------|
| failed\_logins    | _id, timestamp |         |
| logs              | _id, timestamp |         |
| mac\_count        | _id, timestamp |         |
| privileged\_ips   | _id            |         |
| realm\_admins     | _id            |         |
| roaming           | _id, timestamp |         |
| users\_mac        | _id, username  |         |

### Reports

TODO

### Application structure

```
  /home/etlog/etlog           - application root
  |-- app.js                  - main application file, constains appliation configuration
  |-- auth.js                 - authentication configuration
  |-- bin                   
      `-- www                 - script to start the application
  |-- cert                    - certificate related files
  |-- cron                    - cron tasks
      `-- failed_logins.js    - cron task for generating failed_logins collection data
      `-- mac_count.js        - cron task for generating mac_count collection data
      `-- roaming.js          - cron task for generating roaming collection data
      `-- users_mac.js        - cron task for mapping users and mac addresses
  |-- cron.js                 - cron tasks definiton
  |-- db.js                   - database and schema configuration
  |-- doc                     - documentation
  |-- gulpfile.js             - definition of gulp tasks
  |-- mail.js                 - mail api
  |-- mongo_queries           - directory with mongo shell queries for debugging purposes
  |-- node_modules            - application dependency files
  |-- package.json            - definition of application dependencies and properties
  |-- public                  - directory for referring public files
      `-- partials            - directory for generated html files from pug templates
  |-- README.md               - link to doc/notes.md
  |-- request.js              - wrapper to backend api
  |-- routes                  - application routes
  |-- routes.js               - mapping of routes to application
  |-- scripts                 - various scripts
      `-- fticks_to_bson.sh   - transformation script from fticks to bson
      `-- cron.sh             - cron script to import live data delivered by syslog
      `-- old_data.sh         - script to import old data
      `-- process_old_data.js - script to generate database data from old data
  |-- views                   - templates of displayed pages
      `-- templates           - directory with pug templates for html pages
```

### Gulp

Gulp is a build system, which can be used for variuos tasks.

#### Instalation

Gulp must be installed globally by root user by typing:
```
npm install -g gulp-cli
```

#### Usage

Everything that gulp does is defined in gulpfile.js.
After defining tasks, they can be run bu using `gulp`.
Gulp is used to generate html files from pug templating language.
Pug files are in `views/templates/`, html output is in `public/partials`.


### Log files 

Everything related to log files is located in /home/etlog/logs.

```
  /home/etlog/logs           - log files root
  |-- fticks                 - directory with log files and offset files
  |-- last_date              - file with date of last processed log file 
  |-- mongo                  - directory with log files generated by mongoimport
  |-- transform              - directory with files related to transformation from F-Ticks to BSON
      `-- err-*              - file containing line numbers of invalid records
      `-- last_*             - file containing number of last processed line of corresponding log file
  |-- invalid_records        - directory with files containing invalid records for every day
```

#### New data

Incoming syslog data are processed by `scripts/cron.sh` and subsequently by `scripts/fticks_to_bson.sh`.
Data are converted from F-Ticks format (for more see [this](https://tools.ietf.org/id/draft-johansson-fticks-00.html)) to BSON.

Data are processed every 5 minutes by user's crontab.
Last date file (`/home/etlog/logs/last_date`) contains date of last processed log file.
File is updated every day, when the last part of the data is imported.

Last file for every processed log file contains last processed line number.
File on every cron job run. It is used to calculate absolute line numbers for error reporting.

#### Error files

Transform error log like has this structure:
`filename:line number:error reason`
Transform error log file may look like:

```
/home/etlog/logs/fticks/fticks-2016-10-20:681871: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:682504: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:683314: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:684293: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:685727: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:686547: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:688106: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:688122: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:688317: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:689784: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:690431: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:690872: skipped, general error in parsing current record
/home/etlog/logs/fticks/fticks-2016-10-20:692246: skipped, invalid mac address
```

### API

#### api-query-params

For easy use mapping from query string to MongoDB queries is used.
Module api-query-params is used for this functionality.
Official [documentation](https://www.npmjs.com/package/api-query-params) provides full information how to use.
Module is slightly modified to support various timestamps and for correct mapping of them to backend api.

Table below defines operators usage:

| URI                  | example                | explanation            |
|----------------------|------------------------|------------------------|
| `key=val`            | `type=public`          | equal                  |
| `key>val`            | `count>5`              | greater                |
| `key>=val`           | `rating>=9.5`          | greater or equal       |
| `key<val`            | `createdAt<2016-01-01` | lower                  |
| `key<=val`           | `score<=-5`            | lower or equal         |
| `key!=val`           | `status!=success`      | not equal              |
| `key=val1,val2`      | `country=GB,US`        | equal to all listed    |
| `key!=val1,val2`     | `lang!=fr,en`          | not equal to all listed|
| `key`                | `phone`                | exists                 |
| `!key`               | `!email`               | not exists             |
| `key=/value/<opts>`  | `email=/@gmail\.com$/i`| reqex equal            |
| `key!=/value/<opts>` | `phone!=/^06/`         | regex not equal        |

Other operators usage:

| operator type        | example                | explanation                                 |
|----------------------|------------------------|---------------------------------------------|
| skip                 | `skip=10`              | skip 10 items before presenting to the user |
| limit                | `limit=10`             | limit query to 10 items                     |
| sort                 | `sort=key`             | sort ascending by key                       |
| sort                 | `sort=-key`            | sort descending by key                      |
| sort                 | `sort=-key1,-key2`     | sort descending by both key1 and key2       |


Application api:

| URL                             | params | query string variables                                 | note                   |
|---------------------------------|--------|--------------------------------------------------------|------------------------|
| /api/failed\_logins/            |        | timestamp, [ username, fail\_count, ok\_count, ratio ] |                        |
| /api/mac\_count/                |        | timestamp, [ username, count, addrs ]                  |                        |
| /api/roaming/most\_provided/    |        | timestamp, [ inst\_name, provided\_count ]             |                        |
| /api/roaming/most\_used/        |        | timestamp, [ inst\_name, used\_count ]                 |                        |
| /api/saml/metadata              |        |                                                        | url with saml metadata |


### Routes

#### Frontend

TODO

#### Backend

| URL             | explanation                                                        | 
|-----------------|--------------------------------------------------------------------|
| /               | title page                                                         |
| /login          | login page                                                         |
| /login/callback | page where the user is redirected after successful authentication  |
| /auth_fail      | page where the user is redirected when authentication fails        |


##### Examples

Examples below are using the `curl` command, but any other method (wget, browser, ... ) to retrieve http content can be used.
Some of the command below may take some time (units of seconds) to complete.

##### Basic examples

```
curl 'https://etlog.cesnet.cz/api/mac_count/?timestamp=2016-10-07'
curl 'https://etlog.cesnet.cz/api/roaming/most_provided/?timestamp=2016-10-07'
curl 'https://etlog.cesnet.cz/api/roaming/most_used/?timestamp=2016-10-07'
curl 'https://etlog.cesnet.cz/api/failed_logins/?timestamp=2016-10-07'
```

##### Advanced examples

```
# get mac count records for 2016-10-07 with more than 5 mac addresses
curl 'https://etlog.cesnet.cz/api/mac_count/?timestamp=2016-10-07&count>5'

# get mac count records for 2016-10-07 with more than 5 mac addresses, sort from most to least
curl 'https://etlog.cesnet.cz/api/mac_count/?timestamp=2016-10-07&count>5&sort=-count'

# get mac count records for 2016-10-07 with more than mac addresses between 5 and 15, sort from most to least
curl 'https://etlog.cesnet.cz/api/mac_count/?timestamp=2016-10-07&count>5&count<15&sort=-count'

# get most provided roaming records for 2016-10-07 with more than 1000 provided roamings, sort from most to least
curl 'https://etlog.cesnet.cz/api/roaming/most_provided/?timestamp=2016-10-07&provided_count>1000&sort=-count'

# get most used roaming records for 2016-10-07 with more than 100 used roamings, sort from most to least
curl 'https://etlog.cesnet.cz/api/roaming/most_used/?timestamp=2016-10-07&used_count>100&sort=-count'

# get failed logins records for 2016-10-07 with ratio between 0.4 and 0.9, sort from most to least
curl 'https://etlog.cesnet.cz/api/failed_logins/?timestamp=2016-10-07&ratio>0.4&ratio<0.9&sort=-ratio'

# get failed logins records for 2016-10-07 only for users with realms ending '.cz/api' with fail count more than 500, sort from most to least
curl 'https://etlog.cesnet.cz/api/failed_logins/?username=/\.cz$/&timestamp=2016-10-07&fail_count>500&sort=-fail_count'

# get failed logins records from 2016-09-20 to 2016-09-30 only for users with realms ending '.edu'
# with fail count more than 100, sort from most to least
curl 'https://etlog.cesnet.cz/api/failed_logins/?username=/\.edu$/&timestamp>2016-09-20&timestamp<2016-09-30&fail_count>100&sort=-fail_count'

# get failed logins records from 2016-09-20 to 2016-10-10 only for users from realm 'fit.cvut.cz/api'
# with no successful logins, sort from most failed logins to least
# get only 10 results
curl 'https://etlog.cesnet.cz/api/failed_logins/?username=/.*@fit\.cvut\.cz$/&timestamp>2016-09-20&timestamp<2016-10-10&ok_count=0&sort=-fail_count&limit=10'
```


##### Timestamp

Timestamp value must be in one of the formats in table below.
All timestamps used for querying must have set hours, minutes, seconds and milliseconds to 0, when specified.
When not specified, values for hours, minutes, seconds and milliseconds are set automatically to 0.

| format            | example                   |
|-------------------|---------------------------|
| ISO-8601          | 2016-10-06T22:00:00.000Z  |
| reduced ISO-8601  | 2016-10-06T22:00:00       |
| %Y-%m-%d          | 2016-10-06                |


